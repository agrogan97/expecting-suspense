{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Modelling\n",
    "\n",
    "We'll explore 2 networks to see how it can model structure in observations related to suspense and the blackjack game:\n",
    "- A linear network\n",
    "- A recurrent network\n",
    "\n",
    "We'll start by defining a data generation process to use as our training data. For this, we need arrays of 5 card draws as the input, with the corresponding suspense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"maxCard\" : 9,\n",
    "    \"minCard\" : -9,\n",
    "    \"upperBound\" : 10,\n",
    "    \"totalDraws\" : 5\n",
    "}\n",
    "\n",
    "def getCardSequence(params: dict) -> np.ndarray:\n",
    "\n",
    "    return np.asarray([random.randint(params[\"minCard\"], params[\"maxCard\"]) for _ in range(params[\"totalDraws\"])])\n",
    "\n",
    "# Cumulative score, card pair presented -> suspense\n",
    "\n",
    "def getSuspenseSequence(cardSequence: np.array, suspenseModel, params: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Remember, card sequences need to be 5 draws, where before each draw the player is shown 2 cards.\n",
    "    Without this, we're modelling suspense in a different way than the data we're gathering, as effectively \n",
    "    the expected future belief update would involve forming expectations based on the entire deck, not on the 2 \n",
    "    cards they see before they spin the wheel.\n",
    "\n",
    "    We can implement a couple of suspense models here to try different learning functions\n",
    "    For example, we could have:\n",
    "        - A random model\n",
    "        - Ely suspense (L1 norm)\n",
    "        - Distance to upper bound\n",
    "        - Distance to bound modified by number of cards left in sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the cumulative sum of draws\n",
    "    cs = np.cumsum(cardSequence, axis=0, dtype=np.float32)[0:, 0]\n",
    "    # Get the card pairs at each hand\n",
    "    pairs = np.asarray(np.split(cardSequence, params[\"totalDraws\"]), dtype=np.float32).squeeze()\n",
    "    # Store everything in output dict\n",
    "    results = {\n",
    "        \"cs\": cs,\n",
    "        \"pairs\" : pairs,\n",
    "        \"suspense\" : []\n",
    "    }\n",
    "    \n",
    "    if suspenseModel == None:\n",
    "        raise ValueError(\"No suspense model provided.\")\n",
    "    \n",
    "    if suspenseModel == \"random\":\n",
    "        # Randomly generate 5 values\n",
    "        results[\"suspense\"] = np.array([random.randint(1, 5) for _ in range(params[\"totalDraws\"])], dtype=np.float32)\n",
    "    \n",
    "    elif suspenseModel == \"ely\":\n",
    "        # Ely suspense is based on an internal probability model - do we want to give this to the network or let it try and learn this?\n",
    "        # Surely that would be part of the point?\n",
    "        pass\n",
    "    elif suspenseModel == \"toUpperBound\":\n",
    "        # Whichever of the 2 cards is closest to the upper bound, use that difference for suspense\n",
    "        results[\"suspense\"] = 5*np.exp(-np.min((abs(pairs - params[\"upperBound\"])), axis=1)/params[\"upperBound\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Model {suspenseModel} not recognised.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def makeDataset():\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "allX = []\n",
    "allY = []\n",
    "\n",
    "for i in range(1):\n",
    "    cardSeq1 = getCardSequence(params)\n",
    "    cardSeq2 = getCardSequence(params)\n",
    "    # Format into pairs of cards\n",
    "    cardSeq = np.concatenate((\n",
    "        cardSeq1.reshape(-1, 1), cardSeq2.reshape(-1, 1)\n",
    "    ), axis=1)\n",
    "\n",
    "    results = getSuspenseSequence(cardSeq, \"toUpperBound\", params)\n",
    "\n",
    "    # results\n",
    "    regResults = {}\n",
    "\n",
    "    # Normalise data to between -1 and 1\n",
    "    regResults['cs'] = results['cs'] / np.array([35], dtype=np.float32) # 35 being the largest score, w/ a constraint that we can't repeat cards (i.e. 9+8+7+6+5)\n",
    "    regResults['pairs'] = results['pairs'] / np.array([params['maxCard']], dtype=np.float32) # Divide by 9 to normalise\n",
    "\n",
    "    # Format into inputs\n",
    "    allX.append(np.concatenate((regResults['cs'], regResults['pairs'].flatten())))\n",
    "    allY.append(results['suspense'])\n",
    "\n",
    "allX = np.asarray(allX)\n",
    "allY = np.asarray(allY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs': array([ -7., -15., -13., -22., -26.], dtype=float32),\n",
       " 'pairs': array([[-7.,  0.],\n",
       "        [-8.,  8.],\n",
       "        [ 2., -4.],\n",
       "        [-9.,  0.],\n",
       "        [-4., -9.]], dtype=float32),\n",
       " 'suspense': array([1.8393971, 4.0936537, 2.2466447, 1.8393971, 1.2329848],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.0"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['pairs'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.08571429, -0.11428571,  0.        , -0.02857143, -0.2857143 ,\n",
       "          0.33333334, -0.5555556 , -0.7777778 ,  0.11111111,  0.44444445,\n",
       "         -0.5555556 , -0.11111111, -0.6666667 , -1.        ,  0.22222222],\n",
       "        [-0.25714287, -0.2       , -0.2857143 , -0.17142858, -0.11428571,\n",
       "         -1.        , -0.22222222,  0.22222222, -0.44444445, -0.33333334,\n",
       "          0.8888889 ,  0.44444445, -0.7777778 ,  0.22222222,  0.5555556 ],\n",
       "        [-0.2       , -0.14285715, -0.25714287, -0.34285715, -0.42857143,\n",
       "         -0.7777778 ,  0.33333334,  0.22222222,  1.        , -0.44444445,\n",
       "          0.11111111, -0.33333334,  0.5555556 , -0.33333334,  0.33333334],\n",
       "        [ 0.        ,  0.17142858, -0.05714286,  0.11428571,  0.17142858,\n",
       "          0.        , -0.8888889 ,  0.6666667 ,  0.        , -0.8888889 ,\n",
       "         -0.11111111,  0.6666667 , -0.5555556 ,  0.22222222, -0.5555556 ],\n",
       "        [ 0.11428571,  0.02857143, -0.2       ,  0.02857143, -0.14285715,\n",
       "          0.44444445,  0.8888889 , -0.33333334, -0.22222222, -0.8888889 ,\n",
       "          0.11111111,  0.8888889 ,  0.7777778 , -0.6666667 ,  0.        ]],\n",
       "       dtype=float32),\n",
       " array([[2.4829264, 2.0328484, 2.7440581, 1.6643553, 2.2466447],\n",
       "        [1.505971 , 2.2466447, 4.0936537, 2.7440581, 3.0326533],\n",
       "        [2.4829264, 4.524187 , 2.0328484, 3.0326533, 2.4829264],\n",
       "        [1.8393971, 3.3516002, 1.6643553, 3.3516002, 2.2466447],\n",
       "        [4.0936537, 1.505971 , 2.0328484, 4.0936537, 1.8393971]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allX, allY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our inputs and suspenses, we'll build out the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import sklearn.model_selection as skm\n",
    "\n",
    "print(\"Cuda available?\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = skm.train_test_split(allX, allY, test_size=0.2)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([800, 15]),\n",
       " torch.Size([800, 5]),\n",
       " torch.Size([200, 15]),\n",
       " torch.Size([200, 5]))"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP\n",
    "class TorchMLP(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # input layer\n",
    "            torch.nn.Linear(num_features, 15),\n",
    "            torch.nn.Tanh(),\n",
    "\n",
    "            # Hidden layers\n",
    "            torch.nn.Linear(15, 15),\n",
    "            torch.nn.Tanh(),\n",
    "\n",
    "            # Output layer\n",
    "            torch.nn.Linear(15, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloader objects\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train)\n",
    "test_ds = MyDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_ds, batch_size=24, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_ds, batch_size=24, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTotalLoss(model, dataloader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model = model.eval()\n",
    "\n",
    "    loss = 0.0\n",
    "    examples = 0.0\n",
    "\n",
    "    for ix, (features, labels) in enumerate(dataloader):\n",
    "\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            logits = model(features)\n",
    "            batch_loss = F.mse_loss(logits, labels, reduce=\"sum\")\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        examples += logits.shape[0]\n",
    "\n",
    "    return loss / examples\n",
    "\n",
    "def computeAccuracy(model, dataLoader, device=None):\n",
    "    # This won't work without some ammendment as it's expecting categorical data or oh-vectors\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model = model.eval()\n",
    "\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for ix, (features, labels) in enumerate(dataLoader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            logits = model(features)\n",
    "\n",
    "            acc = F.mse_loss(logits, labels, reduction=\"mean\")/(16)\n",
    "    \n",
    "    # print(f\"Correctly answered {correct} out of {total_examples} = {100*correct/total_examples}%\")\n",
    "\n",
    "    return 1 - acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agrog\\Documents\\UCL\\Project\\Code\\expecting-suspense\\.venv\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050  |   Train Batch loss: 0.4748  |   Val Batch loss: 0.5041  |   Val acc: 34.3104%\n",
      "Epoch: 002/050  |   Train Batch loss: 0.2871  |   Val Batch loss: 0.3049  |   Val acc: 61.1992%\n",
      "Epoch: 003/050  |   Train Batch loss: 0.1300  |   Val Batch loss: 0.1398  |   Val acc: 83.1722%\n",
      "Epoch: 004/050  |   Train Batch loss: 0.0857  |   Val Batch loss: 0.0940  |   Val acc: 88.9498%\n",
      "Epoch: 005/050  |   Train Batch loss: 0.0808  |   Val Batch loss: 0.0892  |   Val acc: 89.3475%\n",
      "Epoch: 006/050  |   Train Batch loss: 0.0804  |   Val Batch loss: 0.0888  |   Val acc: 89.2848%\n",
      "Epoch: 007/050  |   Train Batch loss: 0.0803  |   Val Batch loss: 0.0887  |   Val acc: 89.2472%\n",
      "Epoch: 008/050  |   Train Batch loss: 0.0803  |   Val Batch loss: 0.0887  |   Val acc: 89.2322%\n",
      "Epoch: 009/050  |   Train Batch loss: 0.0802  |   Val Batch loss: 0.0886  |   Val acc: 89.2259%\n",
      "Epoch: 010/050  |   Train Batch loss: 0.0802  |   Val Batch loss: 0.0886  |   Val acc: 89.2227%\n",
      "Epoch: 011/050  |   Train Batch loss: 0.0801  |   Val Batch loss: 0.0886  |   Val acc: 89.2206%\n",
      "Epoch: 012/050  |   Train Batch loss: 0.0801  |   Val Batch loss: 0.0885  |   Val acc: 89.2189%\n",
      "Epoch: 013/050  |   Train Batch loss: 0.0801  |   Val Batch loss: 0.0885  |   Val acc: 89.2172%\n",
      "Epoch: 014/050  |   Train Batch loss: 0.0800  |   Val Batch loss: 0.0885  |   Val acc: 89.2156%\n",
      "Epoch: 015/050  |   Train Batch loss: 0.0800  |   Val Batch loss: 0.0885  |   Val acc: 89.2139%\n",
      "Epoch: 016/050  |   Train Batch loss: 0.0800  |   Val Batch loss: 0.0885  |   Val acc: 89.2123%\n",
      "Epoch: 017/050  |   Train Batch loss: 0.0800  |   Val Batch loss: 0.0885  |   Val acc: 89.2107%\n",
      "Epoch: 018/050  |   Train Batch loss: 0.0800  |   Val Batch loss: 0.0884  |   Val acc: 89.2092%\n",
      "Epoch: 019/050  |   Train Batch loss: 0.0799  |   Val Batch loss: 0.0884  |   Val acc: 89.2076%\n",
      "Epoch: 020/050  |   Train Batch loss: 0.0799  |   Val Batch loss: 0.0884  |   Val acc: 89.2062%\n",
      "Epoch: 021/050  |   Train Batch loss: 0.0799  |   Val Batch loss: 0.0884  |   Val acc: 89.2047%\n",
      "Epoch: 022/050  |   Train Batch loss: 0.0799  |   Val Batch loss: 0.0884  |   Val acc: 89.2033%\n",
      "Epoch: 023/050  |   Train Batch loss: 0.0799  |   Val Batch loss: 0.0884  |   Val acc: 89.2020%\n",
      "Epoch: 024/050  |   Train Batch loss: 0.0799  |   Val Batch loss: 0.0884  |   Val acc: 89.2006%\n",
      "Epoch: 025/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1994%\n",
      "Epoch: 026/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1982%\n",
      "Epoch: 027/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1970%\n",
      "Epoch: 028/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1958%\n",
      "Epoch: 029/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1947%\n",
      "Epoch: 030/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1936%\n",
      "Epoch: 031/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1926%\n",
      "Epoch: 032/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1916%\n",
      "Epoch: 033/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1906%\n",
      "Epoch: 034/050  |   Train Batch loss: 0.0798  |   Val Batch loss: 0.0884  |   Val acc: 89.1896%\n",
      "Epoch: 035/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1887%\n",
      "Epoch: 036/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1877%\n",
      "Epoch: 037/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1868%\n",
      "Epoch: 038/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1859%\n",
      "Epoch: 039/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1850%\n",
      "Epoch: 040/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1841%\n",
      "Epoch: 041/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1833%\n",
      "Epoch: 042/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1824%\n",
      "Epoch: 043/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1815%\n",
      "Epoch: 044/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0884  |   Val acc: 89.1806%\n",
      "Epoch: 045/050  |   Train Batch loss: 0.0797  |   Val Batch loss: 0.0885  |   Val acc: 89.1798%\n",
      "Epoch: 046/050  |   Train Batch loss: 0.0796  |   Val Batch loss: 0.0885  |   Val acc: 89.1789%\n",
      "Epoch: 047/050  |   Train Batch loss: 0.0796  |   Val Batch loss: 0.0885  |   Val acc: 89.1780%\n",
      "Epoch: 048/050  |   Train Batch loss: 0.0796  |   Val Batch loss: 0.0885  |   Val acc: 89.1771%\n",
      "Epoch: 049/050  |   Train Batch loss: 0.0796  |   Val Batch loss: 0.0885  |   Val acc: 89.1763%\n",
      "Epoch: 050/050  |   Train Batch loss: 0.0796  |   Val Batch loss: 0.0885  |   Val acc: 89.1754%\n"
     ]
    }
   ],
   "source": [
    "model = TorchMLP(num_features=15)\n",
    "model.to(\"cuda\")\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_ix, (features, labels) in enumerate(train_loader):\n",
    "        features = features.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        logits = model(features)\n",
    "        loss = F.mse_loss(logits, labels, reduction=\"mean\")\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        if batch_ix == 0:\n",
    "            train_loss = computeTotalLoss(model, train_loader, device=\"cuda\")\n",
    "            val_loss = computeTotalLoss(model, test_loader, device=\"cuda\")\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_acc = computeAccuracy(model, test_loader, device=\"cuda\")\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "                f\"  |   Train Batch loss: {train_loss:.4f}\"\n",
    "                f\"  |   Val Batch loss: {val_loss:.4f}\"\n",
    "                f\"  |   Val acc: {100*val_acc:.4f}%\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"Training\")\n",
    "plt.plot(val_losses, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0286,  0.0000,  0.2571,  0.5143,  0.4000, -0.1111,  0.7778,  0.1111,\n",
      "        -0.2222,  1.0000,  0.5556,  1.0000,  0.6667, -0.4444,  0.5556]) tensor([3.7041, 2.0328, 4.5242, 4.5242, 3.0327])\n"
     ]
    }
   ],
   "source": [
    "for ix, (features, labels) in enumerate(test_loader):\n",
    "    print(features[0], labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04600000000000005"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.array([1, 2, 3, 4, 5]) - np.array([1.2, 2.1, 3.1, 4.4, 5.1]))**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2300)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(torch.tensor([1, 2, 3, 4, 5]), torch.tensor([1.2, 2.1, 3.1, 4.4, 5.1]), reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(model, device=\"cuda\"):\n",
    "    params = {\n",
    "        \"maxCard\" : 9,\n",
    "        \"minCard\" : -9,\n",
    "        \"upperBound\" : 10,\n",
    "        \"totalDraws\" : 5\n",
    "    }\n",
    "\n",
    "    # generate 2 sequences:\n",
    "\n",
    "    cardSeq1 = getCardSequence(params)\n",
    "    cardSeq2 = getCardSequence(params)\n",
    "    # Format into pairs of cards\n",
    "    cardSeq = np.concatenate((\n",
    "        cardSeq1.reshape(-1, 1), cardSeq2.reshape(-1, 1)\n",
    "    ), axis=1)\n",
    "\n",
    "    results = getSuspenseSequence(cardSeq, \"toUpperBound\", params)\n",
    "\n",
    "    # encode for network\n",
    "    regResults = {}\n",
    "\n",
    "    # Normalise data to between -1 and 1\n",
    "    regResults['cs'] = results['cs'] / np.array([35], dtype=np.float32) # 35 being the largest score, w/ a constraint that we can't repeat cards (i.e. 9+8+7+6+5)\n",
    "    regResults['pairs'] = results['pairs'] / np.array([params['maxCard']], dtype=np.float32) # Divide by 9 to normalise\n",
    "\n",
    "    allX = []\n",
    "    allY = []\n",
    "\n",
    "    # Format into inputs\n",
    "    allX.append(np.concatenate((regResults['cs'], regResults['pairs'].flatten())))\n",
    "    allY.append(results['suspense'])\n",
    "\n",
    "    X = torch.tensor(allX, dtype=torch.float32)\n",
    "    Y = torch.tensor(allY, dtype=torch.float32)\n",
    "\n",
    "    inf_ds = MyDataset(X, Y)\n",
    "    inf_loader = DataLoader(dataset=inf_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model = model.eval()\n",
    "\n",
    "    for ix, (features, labels) in enumerate(inf_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            logits = model(features)\n",
    "\n",
    "            acc = 1 - F.mse_loss(logits, labels, reduction=\"mean\")/(16)\n",
    "\n",
    "    # print(results)\n",
    "    # print(logits.squeeze())\n",
    "\n",
    "    # Produce summary:\n",
    "    print(\"--- Inference Results ---\\n\")\n",
    "\n",
    "    for cards, cs, sus, modSus in zip(results['pairs'], results['cs'], results['suspense'], logits.squeeze()):\n",
    "        print(f\" Drew: {cards[0]}  |  Alt:  {cards[1]}  |  Sum:  {cs}  |  Suspense:  {sus:.3f}  |  Model Suspense:  {modSus:.3f}  |  Accuracy: {100*acc:.3f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inference Results ---\n",
      "\n",
      " Drew: -9.0  |  Alt:  -7.0  |  Sum:  -9.0  |  Suspense:  0.913  |  Model Suspense:  2.906  |  Accuracy: 92.522%\n",
      " Drew: -8.0  |  Alt:  6.0  |  Sum:  -17.0  |  Suspense:  3.352  |  Model Suspense:  2.934  |  Accuracy: 92.522%\n",
      " Drew: 8.0  |  Alt:  4.0  |  Sum:  -9.0  |  Suspense:  4.094  |  Model Suspense:  2.835  |  Accuracy: 92.522%\n",
      " Drew: 6.0  |  Alt:  -3.0  |  Sum:  -3.0  |  Suspense:  3.352  |  Model Suspense:  2.872  |  Accuracy: 92.522%\n",
      " Drew: 2.0  |  Alt:  4.0  |  Sum:  -1.0  |  Suspense:  2.744  |  Model Suspense:  2.888  |  Accuracy: 92.522%\n"
     ]
    }
   ],
   "source": [
    "demo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(80.)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(torch.tensor([[5, 5, 5, 5, 5]], dtype=torch.float32), torch.tensor([[1, 1, 1, 1, 1]], dtype=torch.float32), reduction=\"sum\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
