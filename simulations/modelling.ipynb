{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful references for this:\n",
    "\n",
    "- https://hmmlearn.readthedocs.io/en/latest/auto_examples/plot_variational_inference.html\n",
    "- https://hmmlearn.readthedocs.io/en/latest/auto_examples/plot_casino.html\n",
    "- https://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a generative model with 2 hidden states: high suspense and low suspense\n",
    "gen_model = hmm.CategoricalHMM(n_components=2, random_state=99)\n",
    "# At the start of the round, they're in a low suspenses state\n",
    "gen_model.startprob_ = np.array([1.0, 0.0])\n",
    "# Let's initialise the transition probability matrix\n",
    "# Part of our experiment is to see if people are keen to reach states of high suspense, \n",
    "# so we could bear that in mind\n",
    "# we'll intialise with 25% chance of staying and 75% of transitioning higher\n",
    "gen_model.transmat_ = np.array([[0.25, 0.75],\n",
    "                               [0.8, 0.2]])\n",
    "# now we can intitialise the emissions matrix\n",
    "# these are the probs of being in each hidden state given the observation\n",
    "# in this initial case, say we observe 4 states: 1, 2, 3, 4 (in reality, we have like 21+ states remember)\n",
    "# We can imagine that 1 and 2 are low suspense, and 3 and 4 are high suspense\n",
    "gen_model.emissionprob_ = np.array([\n",
    "    [0.4, 0.3, 0.2, 0.1], # i.e. the probability im in hidden state 1 (low suspense) given each possible observation\n",
    "    [0.1, 0.2, 0.3, 0.4] # the probability i'm in hidden state 2 for each possible observation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolls, gen_states = gen_model.sample(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #0\tScore: -2079.2023916242306\n",
      "Model #1\tScore: -2085.7473541887784\n",
      "Model #2\tScore: -2076.5715494280234\n",
      "Model #3\tScore: -2081.0994465932536\n",
      "Model #4\tScore: -2079.1353917697265\n",
      "Model #5\tScore: -2074.358454441661\n",
      "Model #6\tScore: -2074.236949701335\n",
      "Model #7\tScore: -2071.8984451128363\n",
      "Model #8\tScore: -2080.968518959379\n",
      "Model #9\tScore: -2080.6361359573903\n",
      "Model #10\tScore: -2081.8742622767954\n",
      "Model #11\tScore: -2076.69108513878\n",
      "Model #12\tScore: -2084.6860628696963\n",
      "Model #13\tScore: -2080.373920441045\n",
      "Model #14\tScore: -2078.118810856695\n",
      "Model #15\tScore: -2081.0933643611447\n",
      "Model #16\tScore: -2080.0528072430097\n",
      "Model #17\tScore: -2079.8051915973524\n",
      "Model #18\tScore: -2078.305514372171\n",
      "Model #19\tScore: -2080.1039744034247\n",
      "Model #20\tScore: -2076.204594353516\n",
      "Model #21\tScore: -2078.0701733149967\n",
      "Model #22\tScore: -2079.4196688870416\n",
      "Model #23\tScore: -2078.9957554343227\n",
      "Model #24\tScore: -2072.616659093793\n",
      "Model #25\tScore: -2075.338893905001\n",
      "Model #26\tScore: -2079.955695350764\n",
      "Model #27\tScore: -2078.212939456073\n",
      "Model #28\tScore: -2080.051581419177\n",
      "Model #29\tScore: -2073.374084638906\n",
      "Model #30\tScore: -2080.057506289375\n",
      "Model #31\tScore: -2081.190213307808\n",
      "Model #32\tScore: -2082.1191135482964\n",
      "Model #33\tScore: -2087.845784985504\n",
      "Model #34\tScore: -2082.941131144541\n",
      "Model #35\tScore: -2082.118715906665\n",
      "Model #36\tScore: -2077.2873330448\n",
      "Model #37\tScore: -2080.4566191835547\n",
      "Model #38\tScore: -2080.4976037917445\n",
      "Model #39\tScore: -2078.5571994046454\n",
      "Model #40\tScore: -2077.7957459913882\n",
      "Model #41\tScore: -2081.38855853642\n",
      "Model #42\tScore: -2079.9427278507924\n",
      "Model #43\tScore: -2081.933158360461\n",
      "Model #44\tScore: -2080.8846002336736\n",
      "Model #45\tScore: -2075.4557379471835\n",
      "Model #46\tScore: -2081.7277501984395\n",
      "Model #47\tScore: -2074.474995547887\n",
      "Model #48\tScore: -2075.2947952031127\n",
      "Model #49\tScore: -2079.5191997863094\n",
      "Generated score: -2072.2922297440637\n",
      "Best score: -2071.8984451128363\n"
     ]
    }
   ],
   "source": [
    "# Do parameter recovery on our initial model\n",
    "\n",
    "# Split data into training and validation\n",
    "x_train = rolls[:rolls.shape[0] // 2]\n",
    "x_validate = rolls[rolls.shape[0] // 2:]\n",
    "\n",
    "# Generate an initial optimal score\n",
    "gen_score = gen_model.score(x_validate)\n",
    "best_score = best_model = None\n",
    "\n",
    "n_fits = 50\n",
    "np.random.seed(13)\n",
    "for ix in range(n_fits):\n",
    "    model = hmm.CategoricalHMM(\n",
    "        n_components=2, n_features=4, random_state=ix, init_params='ste'\n",
    "    )\n",
    "    model.fit(x_train)\n",
    "    score = model.score(x_validate)\n",
    "    print(f'Model #{ix}\\tScore: {score}')\n",
    "    if best_score is None or score > best_score:\n",
    "        best_model = model\n",
    "        best_score = score\n",
    "\n",
    "print(f'Generated score: {gen_score}\\nBest score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = best_model.predict(rolls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transmission Matrix Generated:\n",
      "[[0.25 0.75]\n",
      " [0.8  0.2 ]]\n",
      "\n",
      "Transmission Matrix Recovered:\n",
      "[[0.187 0.813]\n",
      " [0.636 0.364]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Transmission Matrix Generated:\\n{gen_model.transmat_.round(3)}\\n\\n'\n",
    "      f'Transmission Matrix Recovered:\\n{best_model.transmat_.round(3)}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rolls = np.array([[0, 0, 1, 1, 2, 2, 3]])\n",
    "best_model.predict(test_rolls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
